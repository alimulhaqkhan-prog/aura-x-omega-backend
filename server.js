import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import OpenAI from "openai";

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json());

// ------------ OpenAI client (optional) ------------
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
let openaiClient = null;

if (OPENAI_API_KEY) {
  openaiClient = new OpenAI({ apiKey: OPENAI_API_KEY });
  console.log("âœ… OpenAI client initialised");
} else {
  console.log("âš ï¸ OPENAI_API_KEY not set. Backend will return seedReply only.");
}

// ------------ Helper: build system prompt ------------
function buildSystemPrompt(payload) {
  const {
    tm,
    bm,
    D,
    Csum,
    lambdaFaith,
    lambdaSys,
    E0,
    faithLens
  } = payload || {};

  return `
You are **AURA-X Î©**, an *emotional continuity reactor*.

Your job:
- Respect the user's existing seed_reply (it already contains safe advice).
- Slightly refine / polish it, keep the **same meaning**.
- Use maximum 2â€“3 short paragraphs.
- Stay gentle, non-medical, non-therapy. 
- Never claim to cure depression, epilepsy, trauma, etc.

Internal emotional snapshot (from the front-end equation):
- TM  = ${Number(tm ?? 0).toFixed(2)}
- BM  = ${Number(bm ?? 0).toFixed(2)}
- D   = ${Number(D ?? 0).toFixed(2)}
- Î£Câ‚œ = ${Number(Csum ?? 0).toFixed(2)}
- Î»_faith = ${Number(lambdaFaith ?? 0).toFixed(2)}
- Î»_sys   = ${Number(lambdaSys ?? 0).toFixed(2)}
- Eâ‚€      = ${Number(E0 ?? 0).toFixed(2)}

Faith lens selected by user: ${faithLens || "None"}.

Rules:
- If faith lens is set, you may add 1 Ú†Ú¾ÙˆÙ¹ÛŒ Ø¬Ù…Ù„Û Ø§Ø³ faith Ú©Û’ Ø§Ù†Ø¯Ø§Ø² Ù…ÛŒÚº soft encouragement Ú©Û’ Ø·ÙˆØ± Ù¾Ø±Û”
- Ø§Ú¯Ø± faith lens "None" ÛÙˆ ØªÙˆ ØµØ±Ù universal ethics Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºÛ”
- ÛÙ…Ø´Û Ø§Ø­ØªØ±Ø§Ù…ØŒ Ø§Ø­ØªÛŒØ§Ø· Ø§ÙˆØ± kindness maintain Ú©Ø±ÛŒÚºÛ”
`;
}

// ------------ Health / safety guard ------------
function containsCrisisWords(text) {
  if (!text) return false;
  const lower = text.toLowerCase();
  const triggers = [
    "suicide",
    "kill myself",
    "end my life",
    "Ø®ÙˆØ¯Ú©Ø´ÛŒ",
    "Ø§Ù¾Ù†ÛŒ Ø¬Ø§Ù† Ù„Û’",
    "Ø²Ù†Ø¯Ú¯ÛŒ Ø®ØªÙ…"
  ];
  return triggers.some((w) => lower.includes(w));
}

// ------------ Routes ------------

// Simple check (Render health check)
app.get("/", (req, res) => {
  res.send("AURA-X Î© backend is alive âœ…");
});

// Main emotional reaction route
app.post("/api/react", async (req, res) => {
  const body = req.body || {};
  const {
    userText = "",
    seedReply = "",
    analysis = {},
    tm,
    bm,
    D,
    Csum,
    lambdaFaith,
    lambdaSys,
    E0,
    faithLens,
    llmModel
  } = body;

  // 1) Basic safety: Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ Ø¨ÛØª Ø®Ø·Ø±Ù†Ø§Ú© Ø¨Ø§Øª Ù„Ú©Ú¾Û’ ØªÙˆ seedReply override Ú©Ø± Ø¯Ùˆ
  if (containsCrisisWords(userText)) {
    const crisisReply =
      "Ù…ÛŒÚº ØªÙ…ÛØ§Ø±ÛŒ Ø¨Ø§Øª Ø³Ù† Ø±ÛØ§ ÛÙˆÚº Ø§ÙˆØ± Ù…Ø­Ø³ÙˆØ³ ÛÙˆ Ø±ÛØ§ ÛÛ’ Ú©Û ØªÙ… Ø¨ÛØª Ø´Ø¯ÛŒØ¯ emotional Ø¯Ø±Ø¯ Ù…ÛŒÚº ÛÙˆÛ” " +
      "Ù…ÛŒÚº Ø§ÛŒÚ© AI ÛÙˆÚºØŒ Ø§Ø³ Ù„Ø¦Û’ Ø§ÛŒÙ…Ø±Ø¬Ù†Ø³ÛŒ Ù…Ø¯Ø¯ ÛŒØ§ ØªÚ¾Ø±Ø§Ù¾ÛŒ Ù†ÛÛŒÚº Ø¯Û’ Ø³Ú©ØªØ§ØŒ Ù„ÛŒÚ©Ù† Ø¨Ø±Ø§ÛÙ Ú©Ø±Ù… Ú©Ø³ÛŒ Ù‚Ø±ÛŒØ¨ÛŒ Ø§Ù†Ø³Ø§Ù†ØŒ " +
      "ÙÛŒÙ…Ù„ÛŒ Ù…Ù…Ø¨Ø±ØŒ Ø¯ÙˆØ³Øª ÛŒØ§ Ù…Ø³ØªÙ†Ø¯ ÚˆØ§Ú©Ù¹Ø±/Ù…Ø¹Ø§Ù„Ø¬ Ø³Û’ ÙÙˆØ±Ø§Ù‹ Ø±Ø§Ø¨Ø·Û Ú©Ø±ÙˆÛ” Ø§Ú¯Ø± Ø®Ø·Ø±Û ÙÙˆØ±ÛŒ ÛÙˆ ØªÙˆ Ø§Ù¾Ù†Û’ Ù…Ù„Ú© Ú©ÛŒ Ø§ÛŒÙ…Ø±Ø¬Ù†Ø³ÛŒ ÛÛŒÙ„Ù¾ Ù„Ø§Ø¦Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÙˆÛ”";
    return res.json({
      reply: crisisReply,
      provider: null,
      error: null
    });
  }

  // 2) Default answer = seedReply (front-end already Ø¨Ù†Ø§ÛŒØ§ ÛÙˆØ§)
  let finalReply = seedReply || "AURA-X Î© seed reply.";

  // Ø§Ú¯Ø± OpenAI key ÛÛŒ Ù†ÛÛŒÚº ÛÛ’ ØªÙˆ Ø³ÛŒØ¯Ú¾Ø§ seedReply ÙˆØ§Ù¾Ø³ Ú©Ø± Ø¯Ùˆ
  if (!openaiClient) {
    return res.json({
      reply: finalReply + " [Backend: no OPENAI_API_KEY configured, using seed-only mode.]",
      provider: null,
      error: "NO_OPENAI_KEY"
    });
  }

  try {
    const systemPrompt = buildSystemPrompt({
      tm,
      bm,
      D,
      Csum,
      lambdaFaith,
      lambdaSys,
      E0,
      faithLens,
      analysis
    });

    // Ø§Ø¨Ú¾ÛŒ Ú©ÛŒÙ„Ø¦Û’ model hard-code Ø±Ú©Ú¾ÙˆØŒ UI Ø³Û’ Ø¢Ù†Û’ ÙˆØ§Ù„Ø§ llmModel ignore Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº
    const modelName = "gpt-4.1-mini";

    const completion = await openaiClient.chat.completions.create({
      model: modelName,
      messages: [
        { role: "system", content: systemPrompt },
        // User text so LLM Ú©Ùˆ context Ù…Ù„ Ø¬Ø§Ø¦Û’
        {
          role: "user",
          content:
            "User_text:\n" +
            (userText || "User Ù†Û’ Ú©ÙˆØ¦ÛŒ extra text Ù†ÛÛŒÚº Ø¯ÛŒØ§ØŒ ØµØ±Ù TM metadata Ø¨Ú¾ÛŒØ¬Ø§ ÛÛ’Û”")
        },
        // seed reply Ú©Ùˆ previous assistant message Ú©ÛŒ Ø·Ø±Ø­ Ø¯Ùˆ
        {
          role: "assistant",
          content:
            "Existing_seed_reply (improve gently, keep same meaning, max ~3 short paragraphs):\n" +
            (seedReply || "No seed reply, so please just give a short, neutral, kind reaction.")
        }
      ],
      temperature: 0.5,
      max_tokens: 350
    });

    const text =
      completion.choices?.[0]?.message?.content?.trim() || finalReply;

    finalReply = text;

    return res.json({
      reply: finalReply,
      provider: "openai",
      error: null
    });
  } catch (err) {
    console.error("OpenAI error:", err.message);
    return res.json({
      reply:
        finalReply +
        " [AURA-X Î© backend error, falling back to local seed reply. You can continue chatting â€” BM will still save locally.]",
      provider: "openai",
      error: err.message
    });
  }
});

// ------------ Start server ------------
const PORT = process.env.PORT || 10000;
app.listen(PORT, () => {
  console.log(`ðŸš€ AURA-X Î© backend listening on port ${PORT}`);
});
